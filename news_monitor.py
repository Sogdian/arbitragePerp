"""
–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ –º–æ–Ω–µ—Ç —Å –±–∏—Ä–∂
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: Bybit, Gate, MEXC, XT, Binance, Bitget, OKX, BingX
"""
import asyncio
import httpx
import json
import os
from typing import List, Dict, Optional, Union
from datetime import datetime, timedelta, timezone
import logging
import re
from bs4 import BeautifulSoup
from urllib.parse import urlsplit, urlunsplit, urlparse, urljoin

logger = logging.getLogger(__name__)


class NewsMonitor:
    """–ö–ª–∞—Å—Å –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ –º–æ–Ω–µ—Ç"""
    
    def __init__(self):
        # URL —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–¥–¥–µ—Ä–∂–∫–∏/–æ–±—ä—è–≤–ª–µ–Ω–∏–π –±–∏—Ä–∂
        # –î–ª—è –±–∏—Ä–∂ —Å API (Bybit) –∏—Å–ø–æ–ª—å–∑—É–µ–º API. –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö - HTML-—Å–∫—Ä–∞–ø–∏–Ω–≥.
        self.exchange_announcement_urls = {
            # Bybit: –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π API announcements
            # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: https://bybit-exchange.github.io/docs/v5/announcement
            "Bybit": "https://api.bybit.com/v5/announcements/index",
            # MEXC: HTML-—Å–∫—Ä–∞–ø–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            "MEXC": [
                "https://www.mexc.com/ru-RU/announcements/help-faq/deposits-withdrawals-36",
                "https://www.mexc.com/ru-RU/announcements/delistings",
                "https://www.mexc.com/ru-RU/announcements/tag/deposits-withdrawals-36",
            ],
            # Gate.io: HTML-—Å–∫—Ä–∞–ø–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            "Gate": [
                "https://www.gate.com/ru/announcements/deposit-withdrawal",
                "https://www.gate.com/ru/announcements/delisted",
            ],
            # XT.com: HTML-—Å–∫—Ä–∞–ø–∏–Ω–≥ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            "XT": [
                "https://xtsupport.zendesk.com/hc/en-us/sections/360000106872-Announcements",
                "https://www.xt.com/en/support/articles/announcements",
            ],
            # Binance: HTML-—Å–∫—Ä–∞–ø–∏–Ω–≥ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            "Binance": [
                "https://www.binance.com/en/support/announcement",
            ],
            # Bitget: HTML-—Å–∫—Ä–∞–ø–∏–Ω–≥ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            "Bitget": [
                "https://www.bitgetapp.com/support/articles",
                "https://www.bitgetapp.com/support/articles/category/delisting",
            ],
            # OKX: HTML-—Å–∫—Ä–∞–ø–∏–Ω–≥ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            "OKX": [
                "https://www.okx.com/support/hc/en-us/sections/360000030652-Latest-Announcements",
                "https://www.okx.com/support/hc/en-us/categories/115000275432-Announcements",
            ],
            # BingX: HTML-—Å–∫—Ä–∞–ø–∏–Ω–≥ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            "BingX": [
                "https://support.bingx.com/hc/en-us/sections/360000197872-Announcements",
                "https://support.bingx.com/hc/en-us/categories/360000197872-Announcements",
            ],
        }
        
        # –ú–∞–ø–ø–∏–Ω–≥ –Ω–∞–∑–≤–∞–Ω–∏–π –±–∏—Ä–∂ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏—è –≤ —Å–∏—Å—Ç–µ–º–µ
        self.exchange_name_mapping = {
            "bybit": "Bybit",
            "gate": "Gate",
            "mexc": "MEXC",
            "xt": "XT",
            "binance": "Binance",
            "bitget": "Bitget",
            "okx": "OKX",
            "bingx": "BingX",
        }
    
    @staticmethod
    def _dedupe_by_url(items: List[Dict]) -> List[Dict]:
        """–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ URL (—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ—Ä—è–¥–∫–∞)."""
        out: List[Dict] = []
        seen = set()
        for it in items:
            url = (it.get("url") or "").strip()
            key = NewsMonitor._normalize_url(url) if url else None
            if key and key in seen:
                continue
            if key:
                seen.add(key)
            out.append(it)
        return out
    
    @staticmethod
    def _normalize_url(url: str) -> str:
        """–£–±–∏—Ä–∞–µ–º querystring –∏ fragment (utm_*, #hash –∏ —Ç.–ø.) —á—Ç–æ–±—ã –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è/—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–ª–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ."""
        if not url:
            return url
        parts = urlsplit(url)
        # –£–±–∏—Ä–∞–µ–º query –∏ fragment, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º trailing slash (–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è /foo vs /foo/)
        path = parts.path or "/"
        return urlunsplit((parts.scheme, parts.netloc, path, "", ""))
    
    async def _fetch_bybit_announcements(
        self,
        limit: int = 100,
        locale: str = "en-US",
        ann_type: Optional[str] = None,
        tag: Optional[str] = None,
        max_pages: int = 50,
        days_back: int = 60,
    ) -> List[Dict]:
        """
        Bybit official announcements API:
        GET /v5/announcements/index
        
        –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: https://bybit-exchange.github.io/docs/v5/announcement
        """
        timeout = httpx.Timeout(connect=5.0, read=8.0, write=8.0, pool=5.0)
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        base_url = self.exchange_announcement_urls.get("Bybit")
        if not base_url:
            return []
        
        try:
            out: List[Dict] = []
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º UTC –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–∞—Ç
            now_utc = datetime.now(timezone.utc)
            # –ë—É—Ñ–µ—Ä 6 —á–∞—Å–æ–≤, —á—Ç–æ–±—ã –Ω–µ —Ç–µ—Ä—è—Ç—å —Å–æ–±—ã—Ç–∏—è –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ "—Ä–æ–≤–Ω–æ 30 –¥–Ω–µ–π –Ω–∞–∑–∞–¥"
            cutoff_date = now_utc - timedelta(days=days_back, hours=6)
            stop_early = False
            page_limit = min(50, limit)
            
            async with httpx.AsyncClient(timeout=timeout, follow_redirects=True, headers=headers) as client:
                for page in range(1, max_pages + 1):
                    if stop_early:
                        break
                    params: Dict[str, str] = {
                        "locale": locale,
                        "page": str(page),
                        "limit": str(page_limit),  # Bybit –Ω–æ—Ä–º–∞–ª—å–Ω–æ –ø–µ—Ä–µ–≤–∞—Ä–∏–≤–∞–µ—Ç 50
                    }
                    if ann_type:
                        params["type"] = str(ann_type)
                    if tag:
                        params["tag"] = str(tag)
                    
                    r = await client.get(base_url, params=params)
                    if r.status_code != 200:
                        logger.warning("Bybit announcements API –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å %s", r.status_code)
                        break
                    
                    data = r.json()
                    if not isinstance(data, dict) or data.get("retCode") != 0:
                        logger.warning("Bybit announcements API –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É: %s", data.get("retMsg") if isinstance(data, dict) else "bad_json")
                        break
                    
                    result = data.get("result", {})
                    items = result.get("list", []) or []
                    if not items:
                        break
                    
                    for it in items:
                        try:
                            title = (it.get("title") or "").strip()
                            url = self._normalize_url((it.get("url") or "").strip())
                            description = (it.get("description") or "").strip()
                            
                            if not title or not url:
                                continue
                            
                            # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ UTC
                            published_at = now_utc
                            publish_time = it.get("publishTime")
                            if publish_time:
                                try:
                                    published_at = datetime.fromtimestamp(int(str(publish_time)) / 1000, tz=timezone.utc)
                                except Exception:
                                    published_at = now_utc
                            
                            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ (–µ—Å–ª–∏ —ç–ª–µ–º–µ–Ω—Ç —Å—Ç–∞—Ä—à–µ cutoff_date, –º–æ–∂–Ω–æ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å—Å—è)
                            if published_at < cutoff_date:
                                # –°–ø–∏—Å–æ–∫ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–Ω–æ–≤—ã–µ -> —Å—Ç–∞—Ä—ã–µ), –º–æ–∂–Ω–æ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å—Å—è
                                stop_early = True
                                break
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∏–ø –∏ —Ç–µ–≥–∏
                            ann_type_obj = it.get("type", {})
                            type_key = ann_type_obj.get("key", "") if isinstance(ann_type_obj, dict) else ""
                            type_title = ann_type_obj.get("title", "") if isinstance(ann_type_obj, dict) else ""
                            tags_list = it.get("tags", []) or []
                            
                            out.append({
                                "title": title,
                                "body": description[:1000],
                                "url": url,
                                "source": "Bybit",
                                "published_at": published_at,
                                "tags": ["Bybit", "exchange", "announcement", type_key, type_title] + (tags_list if isinstance(tags_list, list) else []),
                            })
                        except Exception:
                            continue
                    
                    out = self._dedupe_by_url(out)
                    if len(out) >= limit:
                        break
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –µ—â–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    total = result.get("total", 0)
                    if page * page_limit >= total:
                        break
                    
                    # –ú—è–≥–∫–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                    await asyncio.sleep(0.05)
            
            out.sort(key=lambda x: x["published_at"], reverse=True)
            out = self._dedupe_by_url(out)[:limit]
            return out
        except Exception as e:
            logger.warning("Bybit announcements API –æ—à–∏–±–∫–∞: %s", e)
            return []
    
    async def _fetch_exchange_announcements(self, limit: int = 100, days_back: int = 60, exchanges: Optional[List[str]] = None) -> List[Dict]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å –±–∏—Ä–∂
        
        Args:
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π
            days_back: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞
            exchanges: –°–ø–∏—Å–æ–∫ –±–∏—Ä–∂ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ["bybit", "gate"]). –ï—Å–ª–∏ None, –ø—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è –≤—Å–µ –±–∏—Ä–∂–∏.
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –±–∏—Ä–∂
        """
        all_news = []
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º UTC –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–∞—Ç
        now_utc = datetime.now(timezone.utc)
        # –ë—É—Ñ–µ—Ä 6 —á–∞—Å–æ–≤, —á—Ç–æ–±—ã –Ω–µ —Ç–µ—Ä—è—Ç—å —Å–æ–±—ã—Ç–∏—è –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ
        lookback = now_utc - timedelta(days=days_back, hours=6)
        
        timeout = httpx.Timeout(connect=5.0, read=8.0, write=8.0, pool=5.0)
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –±–∏—Ä–∂–∏, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã
        exchanges_to_check = self.exchange_announcement_urls
        if exchanges:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –±–∏—Ä–∂ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏—è –≤ —Å–∏—Å—Ç–µ–º–µ
            mapped_exchanges = [self.exchange_name_mapping.get(ex.lower(), ex.capitalize()) for ex in exchanges]
            exchanges_to_check = {name: url for name, url in self.exchange_announcement_urls.items() if name in mapped_exchanges}
        
        async def _fetch_one(exchange_name: str, base_url: Optional[Union[str, List[str]]]) -> List[Dict]:
            local: List[Dict] = []
            try:
                # Bybit ‚Äî –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π JSON API
                if exchange_name == "Bybit":
                    result = await self._fetch_bybit_announcements(
                        limit=min(limit, 200),
                        days_back=days_back,
                        ann_type=None,
                        tag=None,
                    )
                    return result
                
                # –î–ª—è –±–∏—Ä–∂ –±–µ–∑ –ø—É–±–ª–∏—á–Ω–æ–≥–æ API –∏ –±–µ–∑ URL –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                if base_url is None:
                    logger.debug("‚è≠Ô∏è %s: –ø—Ä–æ–ø—É—â–µ–Ω (–Ω–µ—Ç –ø—É–±–ª–∏—á–Ω–æ–≥–æ REST API –¥–ª—è announcements)", exchange_name)
                    return []
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ URL –∫–∞—Ç–µ–≥–æ—Ä–∏–π
                if isinstance(base_url, str):
                    urls_to_fetch = [base_url]
                elif isinstance(base_url, list):
                    urls_to_fetch = base_url
                else:
                    urls_to_fetch = []
                
                if not urls_to_fetch:
                    return []
                
                # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ—Ç—Å–µ–∏–≤–∞–Ω–∏—è –º—É—Å–æ—Ä–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ (–∫–∞—Ç–µ–≥–æ—Ä–∏–∏, —Å–µ–∫—Ü–∏–∏, –ø–æ–∏—Å–∫ –∏ —Ç.–ø.)
                # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –∫ path, –Ω–µ –∫ –ø–æ–ª–Ω–æ–º—É URL
                deny_patterns = [
                    r"/categories?/",
                    r"/sections?/",
                    r"/tag/",
                    r"/search",
                    r"/login",
                    r"/register",
                ]
                deny_re = re.compile("|".join(deny_patterns), re.I)
                
                # seen_urls –Ω–∞ –≤—Å—é –±–∏—Ä–∂—É (–≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏), —á—Ç–æ–±—ã –Ω–µ –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–±–æ—Ç—É
                seen_urls = set()

                for url in urls_to_fetch:
                    try:
                        r = await shared_client.get(url)
                        if r.status_code != 200:
                            logger.debug("üîç %s: announcements %s –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å %s", exchange_name, url, r.status_code)
                            continue
                        
                        soup = BeautifulSoup(r.text, "html.parser")
                        articles: List = []
                        
                        # –û–±—â–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –¥—Ä—É–≥–∏—Ö –±–∏—Ä–∂
                        # –í–∞–∂–Ω–æ: Binance –∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –¥—Ä—É–≥–∏–µ –ø–ª–æ—â–∞–¥–∫–∏ –º–æ–≥—É—Ç –ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –ø–æ—Å—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ /square/post/...
                        articles.extend(soup.find_all("a", href=re.compile(r"article|announcement|support|help|square|post", re.I)))
                        articles.extend(soup.find_all(["article", "div"], class_=re.compile(r"article|announcement|news|support", re.I)))
                        
                        # –ñ—ë—Å—Ç–∫–∏–π –ø–æ—Ç–æ–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç–µ–π (–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å)
                        max_articles = min(2000, max(200, limit * 10))
                        for article in articles[:max_articles]:
                            try:
                                url_elem = article if getattr(article, "name", None) == "a" else article.find("a")
                                if not url_elem:
                                    continue
                                href = url_elem.get("href", "") or ""
                                if not href:
                                    continue
                                if not href.startswith("http"):
                                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º urljoin –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–π —Å–±–æ—Ä–∫–∏ URL (–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç edge-cases)
                                    href = urljoin(url, href)

                                # fragment –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –Ω–∞ —Å–µ—Ä–≤–µ—Ä, –ø–æ—ç—Ç–æ–º—É —É–±–∏—Ä–∞–µ–º; query –æ—Å—Ç–∞–≤–ª—è–µ–º (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤–∞–∂–µ–Ω –¥–ª—è –ª–æ–∫–∞–ª–∏/–º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏)
                                href = href.split("#")[0].strip()
                                if not href:
                                    continue

                                href_key = self._normalize_url(href)
                                
                                # –§–∏–ª—å—Ç—Ä—É–µ–º –º—É—Å–æ—Ä–Ω—ã–µ —Å—Å—ã–ª–∫–∏ (–∫–∞—Ç–µ–≥–æ—Ä–∏–∏, —Å–µ–∫—Ü–∏–∏, –ø–æ–∏—Å–∫ –∏ —Ç.–ø.)
                                # –ü—Ä–∏–º–µ–Ω—è–µ–º deny —Ç–æ–ª—å–∫–æ –∫ path, —á—Ç–æ–±—ã –Ω–µ –≤—ã–∫–∏–Ω—É—Ç—å –≤–∞–ª–∏–¥–Ω—ã–µ /support/... –∏–ª–∏ /help/...
                                parsed = urlparse(href)
                                if deny_re.search(parsed.path):
                                    continue
                                
                                if href_key in seen_urls:
                                    continue
                                seen_urls.add(href_key)
                    
                                title_elem = article.find(["h1", "h2", "h3", "h4", "span", "div", "a"], class_=re.compile(r"title|heading|name", re.I))
                                if not title_elem:
                                    title_elem = url_elem
                                title = (title_elem.get_text(strip=True) if title_elem else "").strip()
                                if not title or len(title) < 5:
                                    continue
                                
                                body_elem = article.find(["p", "div", "span"], class_=re.compile(r"content|body|description|text|summary", re.I))
                                body = body_elem.get_text(strip=True)[:500] if body_elem else ""
                                
                                # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∏–∑ —Å—Ç–∞—Ç—å–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Å–ø–∏—Å–∫–∞
                                published_at = None
                                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –¥–∞—Ç—É –≤ time —ç–ª–µ–º–µ–Ω—Ç–µ —Ä—è–¥–æ–º —Å article
                                time_elem = article.find("time")
                                if time_elem:
                                    datetime_attr = time_elem.get("datetime")
                                    if datetime_attr:
                                        try:
                                            # –ü–∞—Ä—Å–∏–º ISO —Ñ–æ—Ä–º–∞—Ç: 2024-01-15T10:30:00Z –∏–ª–∏ 2024-01-15T10:30:00+00:00
                                            if "T" in datetime_attr:
                                                published_at = datetime.fromisoformat(datetime_attr.replace("Z", "+00:00"))
                                            else:
                                                published_at = datetime.strptime(datetime_attr, "%Y-%m-%d")
                                                published_at = published_at.replace(tzinfo=timezone.utc)
                                        except Exception:
                                            pass
                                
                                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ time, –∏—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ —Ä—è–¥–æ–º (–º–Ω–æ–≥–∏–µ –±–∏—Ä–∂–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –¥–∞—Ç—É –≤ span/div)
                                if published_at is None:
                                    date_elem = article.find(["span", "div", "p"], class_=re.compile(r"date|time|published|created", re.I))
                                    if date_elem:
                                        date_text = date_elem.get_text(strip=True)
                                        # –ü—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç
                                        for fmt in ["%Y-%m-%d", "%d.%m.%Y", "%m/%d/%Y", "%Y/%m/%d"]:
                                            try:
                                                published_at = datetime.strptime(date_text[:10], fmt)
                                                published_at = published_at.replace(tzinfo=timezone.utc)
                                                break
                                            except Exception:
                                                continue
                                
                                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞—Ç—É –∫ UTC
                                published_at_inferred = False
                                if published_at is not None:
                                    if published_at.tzinfo is not None:
                                        published_at = published_at.astimezone(timezone.utc)
                                    else:
                                        # –ï—Å–ª–∏ –¥–∞—Ç–∞ –±–µ–∑ timezone, —Å—á–∏—Ç–∞–µ–º —á—Ç–æ UTC
                                        published_at = published_at.replace(tzinfo=timezone.utc)
                                else:
                                    # –Ω–µ—Ç –¥–∞—Ç—ã => –æ—Å—Ç–∞–≤–ª—è–µ–º, –∏–Ω–∞—á–µ days_back –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –±–∏—Ä–∂–∞—Ö –±–µ–∑ –¥–∞—Ç –≤ –ª–∏—Å—Ç–∏–Ω–≥–µ
                                    # —Å—Ç–∞–≤–∏–º now_utc —á—Ç–æ–±—ã —ç–ª–µ–º–µ–Ω—Ç –ø—Ä–æ—à—ë–ª —Ñ–∏–ª—å—Ç—Ä, –¥–∞—Ç—É –ø–æ–ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–∏ –¥–æ–≥—Ä—É–∑–µ
                                    published_at = datetime.now(timezone.utc)
                                    published_at_inferred = True
                                
                                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ lookback —Å—Ä–∞–∑—É (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)
                                # –ù–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∏–ª—å—Ç—Ä –¥–ª—è inferred –¥–∞—Ç, —á—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –±–µ–∑ –¥–∞—Ç—ã
                                if not published_at_inferred and published_at <= lookback:
                                    continue
                                
                                local.append(
                                    {
                                        "title": title,
                                        "body": body,
                                        "url": href,  # –≤–∞–∂–Ω–æ: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π URL (—Å query), –¥–µ–¥—É–ø –¥–µ–ª–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ
                                        "source": exchange_name,
                                        "published_at": published_at,
                                        "published_at_inferred": published_at_inferred,
                                        "tags": [exchange_name, "exchange", "announcement"],
                                    }
                                )
                                if len(local) >= limit:
                                    break
                            except Exception:
                                continue
                    except Exception as e:
                        logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ URL {url} –¥–ª—è {exchange_name}: {e}")
                        continue
                
                if local:
                    logger.debug("  ‚úì %s: –∑–∞–≥—Ä—É–∂–µ–Ω–æ %s –æ–±—ä—è–≤–ª–µ–Ω–∏–π", exchange_name, len(local))
                # –î–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ–º –ø–æ URL –ø–µ—Ä–µ–¥ –≤–æ–∑–≤—Ä–∞—Ç–æ–º
                local = self._dedupe_by_url(local)
                return local[:limit]
            except Exception as e:
                logger.warning("‚ùå %s: –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ announcements: %s", exchange_name, e)
                return []
        
        async with httpx.AsyncClient(timeout=timeout, follow_redirects=True, headers=headers) as shared_client:
            tasks = [_fetch_one(name, url) for name, url in exchanges_to_check.items() if url is not None]
            chunks = await asyncio.gather(*tasks, return_exceptions=False)
            for chunk in chunks:
                all_news.extend(chunk)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–∑–∞—â–∏—Ç–∞ –æ—Ç naive datetime) + —Å–æ—Ä—Ç–∏—Ä—É–µ–º + –¥–µ–¥—É–ø
        filtered: List[Dict] = []
        for n in all_news:
            dt = n.get("published_at")
            if not isinstance(dt, datetime):
                continue
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
                n["published_at"] = dt
            if dt > lookback:
                filtered.append(n)
        
        # –°–Ω–∞—á–∞–ª–∞ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–Ω–æ–≤—ã–µ -> —Å—Ç–∞—Ä—ã–µ)
        filtered.sort(
            key=lambda x: x.get("published_at") or datetime.min.replace(tzinfo=timezone.utc),
            reverse=True,
        )
        # –ó–∞—Ç–µ–º —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–æ–π —É–±–∏—Ä–∞–µ–º inferred –Ω–∏–∂–µ (–Ω–µ "–≤—ã–¥–∞–≤–ª–∏–≤–∞–µ–º" —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞—Ç—ã)
        filtered.sort(key=lambda x: bool(x.get("published_at_inferred", False)))
        
        # –î–µ–¥—É–ø –ø–æ URL –Ω–∞ —É—Ä–æ–≤–Ω–µ –≤—Å–µ—Ö –±–∏—Ä–∂ (–ø–æ—Å–ª–µ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏, —á—Ç–æ–±—ã –æ—Å—Ç–∞–≤–∞–ª—Å—è —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π)
        filtered = self._dedupe_by_url(filtered)
        return filtered[:limit]
    
    async def find_delisting_news(self, news: List[Dict], coin_symbol: str, lookback: Optional[datetime] = None) -> List[Dict]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ –º–æ–Ω–µ—Ç—ã –Ω–∞ –±–∏—Ä–∂–∞—Ö
        
        Args:
            news: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
            coin_symbol: –°–∏–º–≤–æ–ª –º–æ–Ω–µ—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, "FLOW", "BTC")
            lookback: –î–∞—Ç–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ days_back (–µ—Å–ª–∏ None, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è)
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ
        """
        coin_upper = coin_symbol.upper()
        relevant_news = []
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–∞: —Ä–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ hard (—Ä–µ–∞–ª—å–Ω—ã–π –¥–µ–ª–∏—Å—Ç–∏–Ω–≥) –∏ soft (–≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞—É–∑–∞)
        hard_delisting_keywords = [
            "delist", "delisting", "removal", "removed", "discontinued", "terminated",
            "will be delisted", "to be delisted", "delisting announcement",
            "removal from trading", "cease trading", "termination",
            "—É–¥–∞–ª–µ–Ω–∏–µ", "–¥–µ–ª–∏—Å—Ç–∏–Ω–≥", "–ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤–ª–∏", "—É–¥–∞–ª–µ–Ω–∏–µ —Å –±–∏—Ä–∂–∏",
            "–ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏–µ –ª–∏—Å—Ç–∏–Ω–≥–∞", "–∏—Å–∫–ª—é—á–µ–Ω–∏–µ –∏–∑ —Ç–æ—Ä–≥–æ–≤–ª–∏"
        ]
        # soft_keywords (suspend/halt/pause) - –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—É–∑—ã, –Ω–µ —Å—á–∏—Ç–∞–µ–º –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–æ–º
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ hard-–Ω–∞–±–æ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–∞
        delisting_keywords = hard_delisting_keywords
        
        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –º–æ–Ω–µ—Ç—ã
        # –§—å—é—á–µ—Ä—Å—ã —Ç–æ–ª—å–∫–æ –∫ USDT, –ø–æ—ç—Ç–æ–º—É –∏—â–µ–º OBOL –∏ OBOLUSDT
        coin_pattern = re.compile(
            rf"(?<![A-Z0-9]){re.escape(coin_upper)}(?:USDT)?(?![A-Z0-9])",
            re.IGNORECASE
        )
        
        # –£—Å–ª–æ–≤–Ω—ã–π –¥–æ–≥—Ä—É–∑ —Å—Ç–∞—Ç–µ–π: –¥–æ–≥—Ä—É–∂–∞–µ–º –µ—Å–ª–∏ –º–æ–Ω–µ—Ç–∞ —É–ø–æ–º—è–Ω—É—Ç–∞ –∏–ª–∏ –µ—Å—Ç—å delist-–∫–ª—é—á–∏ –≤ –∫–∞—Ä—Ç–æ—á–∫–µ
        timeout = httpx.Timeout(connect=5.0, read=8.0, write=8.0, pool=5.0)
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        
        # –ó–∞—â–∏—Ç–Ω—ã–µ –º–µ—Ä—ã –¥–ª—è –¥–æ–≥—Ä—É–∑–∞: –ª–∏–º–∏—Ç, –∫–µ—à (—Ö—Ä–∞–Ω–∏—Ç body –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å)
        fetch_cache: Dict[str, Optional[str]] = {}  # –∫–µ—à –ø–æ URL: body –∏–ª–∏ None (sentinel –¥–ª—è "–Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å")
        fetch_limit = 20  # –¥–æ–≥—Ä—É–∂–∞—Ç—å –º–∞–∫—Å–∏–º—É–º 20 —Å—Ç–∞—Ç–µ–π –Ω–∞ –º–æ–Ω–µ—Ç—É
        
        # –û–¥–∏–Ω –∫–ª–∏–µ–Ω—Ç –Ω–∞ –≤—Å—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
        async with httpx.AsyncClient(timeout=timeout, follow_redirects=True, headers=headers) as client:
            fetch_count = 0
            for article in news:
                # –†–∞–Ω–Ω–∏–π skip: –µ—Å–ª–∏ lookback –∑–∞–¥–∞–Ω –∏ —É —Å—Ç–∞—Ç—å–∏ —É–∂–µ –µ—Å—Ç—å "—Ç–≤—ë—Ä–¥–∞—è" –¥–∞—Ç–∞ (–Ω–µ inferred) –∏ –æ–Ω–∞ —Å—Ç–∞—Ä–∞—è
                if lookback is not None and not article.get("published_at_inferred", False):
                    published_at = article.get("published_at")
                    if isinstance(published_at, datetime):
                        # –ó–∞—â–∏—Ç–∞: –µ—Å–ª–∏ published_at naive, —Å—á–∏—Ç–∞–µ–º —á—Ç–æ UTC
                        if published_at.tzinfo is None:
                            published_at = published_at.replace(tzinfo=timezone.utc)
                        if published_at <= lookback:
                            continue
                
                # –õ–æ–∫–∞–ª—å–Ω—ã–µ (–±–µ–∑ —Å–∞–π–¥-—ç—Ñ—Ñ–µ–∫—Ç–æ–≤): –∞–∫—Ç—É–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞/–ø—Ä–∏–∑–Ω–∞–∫ inferred –¥–ª—è —ç—Ç–æ–π —Å—Ç–∞—Ç—å–∏
                effective_published_at = article.get("published_at")
                if isinstance(effective_published_at, datetime) and effective_published_at.tzinfo is None:
                    effective_published_at = effective_published_at.replace(tzinfo=timezone.utc)
                effective_published_at_inferred = bool(article.get("published_at_inferred", False))

                title_body = (article.get("title", "") + " " + article.get("body", "")).upper()
                tags_upper = [str(t).upper() for t in article.get("tags", [])]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –º–æ–Ω–µ—Ç—ã (—Å —É—á–µ—Ç–æ–º —Å—É—Ñ—Ñ–∏–∫—Å–∞ USDT, —Ç–∞–∫ –∫–∞–∫ —Ñ—å—é—á–µ—Ä—Å—ã —Ç–æ–ª—å–∫–æ –∫ USDT)
                # –ù–∞—Ö–æ–¥–∏—Ç OBOL –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ, –∏ OBOLUSDT
                coin_mentioned = coin_pattern.search(title_body) is not None
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ –≤ –∫–∞—Ä—Ç–æ—á–∫–µ
                has_delisting_keywords_in_card = any(keyword.upper() in title_body for keyword in delisting_keywords)

                # –°–∏–ª—å–Ω—ã–π early-skip: –µ—Å–ª–∏ –¥–∞—Ç–∞ inferred –∏ –Ω–µ—Ç –Ω–∏ –º–æ–Ω–µ—Ç—ã, –Ω–∏ delist-–∫–ª—é—á–µ–π –≤ –∫–∞—Ä—Ç–æ—á–∫–µ ‚Äî —Å–º—ã—Å–ª–∞ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –Ω–µ—Ç
                if effective_published_at_inferred and (not coin_mentioned) and (not has_delisting_keywords_in_card):
                    continue
                
                # –£—Å–ª–æ–≤–Ω—ã–π –¥–æ–≥—Ä—É–∑: –µ—Å–ª–∏ –º–æ–Ω–µ—Ç–∞ —É–ø–æ–º—è–Ω—É—Ç–∞ –ò–õ–ò –µ—Å—Ç—å delist-–∫–ª—é—á–∏ –≤ –∫–∞—Ä—Ç–æ—á–∫–µ (–¥–∞–∂–µ –±–µ–∑ –º–æ–Ω–µ—Ç—ã)
                # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å "batch delisting" –Ω–æ–≤–æ—Å—Ç–∏, –≥–¥–µ –º–æ–Ω–µ—Ç–∞ —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏ —Å—Ç–∞—Ç—å–∏
                should_fetch = (coin_mentioned and not has_delisting_keywords_in_card) or (has_delisting_keywords_in_card and not coin_mentioned)
                
                if should_fetch and fetch_count < fetch_limit:
                    article_url = article.get("url", "")
                    if article_url and article_url.startswith("http"):
                        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL –¥–ª—è –∫–µ—à–∞ (–∫–∞–∫ –≤ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ç–æ—Ä–µ)
                        article_url_normalized = self._normalize_url(article_url)
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
                        if article_url_normalized in fetch_cache:
                            cached_body = fetch_cache[article_url_normalized]
                            if cached_body is not None:  # None –æ–∑–Ω–∞—á–∞–µ—Ç "–Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å", –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å
                                title_body = (article.get("title", "") + " " + cached_body).upper()
                                # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º coin_mentioned –ø–æ—Å–ª–µ –¥–æ–≥—Ä—É–∑–∞
                                coin_mentioned = coin_pattern.search(title_body) is not None
                        else:
                            try:
                                # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π URL (query –º–æ–∂–µ—Ç –±—ã—Ç—å –≤–∞–∂–µ–Ω –¥–ª—è –ª–æ–∫–∞–ª–∏/–º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏),
                                # –∑–∞—Ç–µ–º fallback –Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π (–µ—Å–ª–∏ original 4xx/5xx).
                                r = await client.get(article_url)
                                fetch_count += 1

                                if (
                                    r.status_code >= 400
                                    and fetch_count < fetch_limit
                                    and article_url_normalized != article_url
                                ):
                                    r = await client.get(article_url_normalized)
                                    fetch_count += 1
                                
                                if r.status_code == 200:
                                    soup_article = BeautifulSoup(r.text, "html.parser")
                                    
                                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –¥–∞—Ç—É –∏–∑ —Å—Ç–∞—Ç—å–∏ (–¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è days_back —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)
                                    published_at_updated = None
                                    
                                    # 1) –ü—Ä–æ–±—É–µ–º <time datetime>
                                    time_elem = soup_article.find("time")
                                    if time_elem:
                                        datetime_attr = time_elem.get("datetime")
                                        if datetime_attr:
                                            try:
                                                if "T" in datetime_attr:
                                                    published_at_updated = datetime.fromisoformat(datetime_attr.replace("Z", "+00:00"))
                                                else:
                                                    published_at_updated = datetime.strptime(datetime_attr, "%Y-%m-%d")
                                                    published_at_updated = published_at_updated.replace(tzinfo=timezone.utc)
                                            except Exception:
                                                pass
                                    
                                    # 2) –ü—Ä–æ–±—É–µ–º meta —Ç–µ–≥–∏ (article:published_time, og:published_time, publish-date –∏ —Ç.–ø.)
                                    if published_at_updated is None:
                                        for prop in ["article:published_time", "og:published_time", "publish-date", "datePublished"]:
                                            meta_published = soup_article.find("meta", property=prop) or soup_article.find("meta", attrs={"name": prop})
                                            if meta_published:
                                                content = meta_published.get("content", "")
                                                if content:
                                                    try:
                                                        published_at_updated = datetime.fromisoformat(content.replace("Z", "+00:00"))
                                                        break
                                                    except Exception:
                                                        continue
                                    
                                    # 3) –ü—Ä–æ–±—É–µ–º JSON-LD (schema.org Article)
                                    if published_at_updated is None:
                                        json_ld_scripts = soup_article.find_all("script", type="application/ld+json")
                                        for script in json_ld_scripts:
                                            # script.string —á–∞—Å—Ç–æ None, –∞ JSON-LD –±—ã–≤–∞–µ—Ç –≤ script.get_text()
                                            raw = (script.string or script.get_text() or "").strip()
                                            if not raw:
                                                continue
                                            try:
                                                data = json.loads(raw)
                                            except Exception:
                                                continue
                                            
                                            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ candidate-–æ–±—ä–µ–∫—Ç—ã (dict, list, @graph)
                                            objs = []
                                            if isinstance(data, dict):
                                                objs.append(data)
                                                if isinstance(data.get("@graph"), list):
                                                    objs.extend([x for x in data["@graph"] if isinstance(x, dict)])
                                            elif isinstance(data, list):
                                                objs.extend([x for x in data if isinstance(x, dict)])
                                                for x in data:
                                                    if isinstance(x, dict) and isinstance(x.get("@graph"), list):
                                                        objs.extend([y for y in x["@graph"] if isinstance(y, dict)])
                                            
                                            # –ò—â–µ–º –¥–∞—Ç—É –≤ –ª—é–±–æ–º –æ–±—ä–µ–∫—Ç–µ (datePublished/dateCreated/dateModified).
                                            for obj in objs:
                                                obj_type_val = obj.get("@type")
                                                obj_type = ""
                                                if isinstance(obj_type_val, str):
                                                    obj_type = obj_type_val.lower()
                                                elif isinstance(obj_type_val, list):
                                                    obj_type = " ".join(str(x).lower() for x in obj_type_val)

                                                # –ï—Å–ª–∏ @type —É–∫–∞–∑–∞–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —è–≤–Ω–æ –Ω–µ-—Å—Ç–∞—Ç—å–∏, —á—Ç–æ–±—ã –Ω–µ –ª–æ–≤–∏—Ç—å "–¥–∞—Ç—ã —Å–∞–π—Ç–∞"
                                                if obj_type and not any(k in obj_type for k in ("article", "newsarticle", "blog", "posting")):
                                                    continue

                                                date_str = obj.get("datePublished") or obj.get("dateCreated") or obj.get("dateModified")
                                                if date_str:
                                                    try:
                                                        published_at_updated = datetime.fromisoformat(str(date_str).replace("Z", "+00:00"))
                                                        break
                                                    except Exception:
                                                        continue
                                            
                                            if published_at_updated is not None:
                                                break
                                    
                                    # –û–±–Ω–æ–≤–ª—è–µ–º published_at –≤ —Å—Ç–∞—Ç—å–µ, –µ—Å–ª–∏ –Ω–∞—à–ª–∏ –¥–∞—Ç—É
                                    if published_at_updated is not None:
                                        if published_at_updated.tzinfo is not None:
                                            published_at_updated = published_at_updated.astimezone(timezone.utc)
                                        else:
                                            published_at_updated = published_at_updated.replace(tzinfo=timezone.utc)
                                        effective_published_at = published_at_updated
                                        effective_published_at_inferred = False
                                        
                                        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ lookback –ø–æ—Å–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞—Ç—ã
                                        if lookback is not None and published_at_updated <= lookback:
                                            continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–∞—Ä—É—é –Ω–æ–≤–æ—Å—Ç—å
                                    
                                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
                                    main_content = soup_article.find("main") or soup_article.find("article") or soup_article.find("div", class_=re.compile(r"content|article|body", re.I))
                                    if main_content:
                                        body_full = main_content.get_text(strip=True)[:2000]  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
                                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
                                        fetch_cache[article_url_normalized] = body_full
                                        # –ù–ï –º—É—Ç–∏—Ä—É–µ–º article, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                                        title_body = (article.get("title", "") + " " + body_full).upper()
                                        # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º coin_mentioned –ø–æ—Å–ª–µ –¥–æ–≥—Ä—É–∑–∞
                                        coin_mentioned = coin_pattern.search(title_body) is not None
                                    else:
                                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º None –∫–∞–∫ sentinel - –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å
                                        fetch_cache[article_url_normalized] = None
                            except Exception as e:
                                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–≥—Ä—É–∑–∏—Ç—å —Å—Ç–∞—Ç—å—é {article_url}: {e}")
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º None –≤ –∫–µ—à, —á—Ç–æ–±—ã –Ω–µ –ø–æ–≤—Ç–æ—Ä—è—Ç—å –∑–∞–ø—Ä–æ—Å
                                fetch_cache[article_url_normalized] = None
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ –∏–ª–∏ —è–≤–Ω–æ–≥–æ annType=symbol_delisting
                has_delisting_keywords = any(keyword.upper() in title_body for keyword in delisting_keywords) or ("SYMBOL_DELISTING" in tags_upper)
                
                # –õ–æ–≥–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏, –µ—Å–ª–∏ –º–æ–Ω–µ—Ç–∞ —É–ø–æ–º—è–Ω—É—Ç–∞, –Ω–æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω
                if coin_mentioned and not has_delisting_keywords:
                    logger.info(f"–ú–æ–Ω–µ—Ç–∞ {coin_symbol} –Ω–∞–π–¥–µ–Ω–∞ –≤ '{article.get('title', '')[:60]}...', –Ω–æ –Ω–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–∞")
                
                if coin_mentioned and has_delisting_keywords:
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥ –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ
                    article_with_tag = article.copy()
                    if "delisting" not in article_with_tag.get("tags", []):
                        # –í–∞–∂–Ω–æ: –Ω–µ –º—É—Ç–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ tags –≤ –∏—Å—Ö–æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–µ (shallow copy)
                        tags = list(article_with_tag.get("tags", []) or [])
                        tags.append("delisting")
                        article_with_tag["tags"] = tags
                    # –ù–µ –º—É—Ç–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π article: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –ø–æ–ª—è —Ç–æ–ª—å–∫–æ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
                    article_with_tag["published_at"] = effective_published_at
                    article_with_tag["published_at_inferred"] = effective_published_at_inferred
                    relevant_news.append(article_with_tag)
                    # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–π –¥–µ–ª–∏—Å—Ç–∏–Ω–≥ —Å URL
                    url = article.get('url', 'N/A')
                    logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω –¥–µ–ª–∏—Å—Ç–∏–Ω–≥ {coin_symbol}: {article.get('title', '')[:80]}... | URL: {url}")
        
        return relevant_news
    
    async def check_delisting(self, coin_symbol: str, exchanges: Optional[List[str]] = None, days_back: int = 60) -> List[Dict]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ –º–æ–Ω–µ—Ç—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π
        
        Args:
            coin_symbol: –°–∏–º–≤–æ–ª –º–æ–Ω–µ—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, "DGRAM", "IOTA")
            exchanges: –°–ø–∏—Å–æ–∫ –±–∏—Ä–∂ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ["bybit", "gate"]). –ï—Å–ª–∏ None, –ø—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è –≤—Å–µ –±–∏—Ä–∂–∏. –ï—Å–ª–∏ [], –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è.
            days_back: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 60)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ
        """
        # [] => —è–≤–Ω–æ –Ω–∏—á–µ–≥–æ –Ω–µ –ø—Ä–æ–≤–µ—Ä—è—Ç—å
        if exchanges == []:
            return []
        
        # –ü–æ–ª—É—á–∞–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å –±–∏—Ä–∂ (None => –≤—Å–µ –±–∏—Ä–∂–∏)
        all_announcements = await self._fetch_exchange_announcements(limit=200, days_back=days_back, exchanges=exchanges)
        
        # –í—ã—á–∏—Å–ª—è–µ–º lookback –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –¥–æ–≥—Ä—É–∑–∞ –¥–∞—Ç—ã
        now_utc = datetime.now(timezone.utc)
        lookback = now_utc - timedelta(days=days_back, hours=6) if days_back > 0 else None
        
        # –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ (—Ç–µ–ø–µ—Ä—å async –¥–ª—è —É—Å–ª–æ–≤–Ω–æ–≥–æ –¥–æ–≥—Ä—É–∑–∞ —Å—Ç–∞—Ç–µ–π)
        delisting_news = await self.find_delisting_news(all_announcements, coin_symbol, lookback=lookback)
        
        return delisting_news

