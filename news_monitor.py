"""
–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ –º–æ–Ω–µ—Ç —Å –±–∏—Ä–∂
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: Bybit, Gate, MEXC, LBank
"""
import asyncio
import httpx
from typing import List, Dict, Optional, Union
from datetime import datetime, timedelta, timezone
import logging
import re
from bs4 import BeautifulSoup
from urllib.parse import urlsplit, urlunsplit

logger = logging.getLogger(__name__)


class NewsMonitor:
    """–ö–ª–∞—Å—Å –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ –º–æ–Ω–µ—Ç"""
    
    def __init__(self):
        # URL —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–¥–¥–µ—Ä–∂–∫–∏/–æ–±—ä—è–≤–ª–µ–Ω–∏–π –±–∏—Ä–∂
        # –î–ª—è –±–∏—Ä–∂ —Å API (Bybit) –∏—Å–ø–æ–ª—å–∑—É–µ–º API. –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö - HTML-—Å–∫—Ä–∞–ø–∏–Ω–≥.
        self.exchange_announcement_urls = {
            # Bybit: –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π API announcements
            # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: https://bybit-exchange.github.io/docs/v5/announcement
            "Bybit": "https://api.bybit.com/v5/announcements/index",
            # MEXC: HTML-—Å–∫—Ä–∞–ø–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            "MEXC": [
                "https://www.mexc.com/ru-RU/announcements/help-faq/deposits-withdrawals-36",
                "https://www.mexc.com/ru-RU/announcements/delistings",
                "https://www.mexc.com/ru-RU/announcements/tag/deposits-withdrawals-36",
            ],
            # Gate.io: HTML-—Å–∫—Ä–∞–ø–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            "Gate": [
                "https://www.gate.com/ru/announcements/deposit-withdrawal",
                "https://www.gate.com/ru/announcements/delisted",
            ],
        }
    
    @staticmethod
    def _dedupe_by_url(items: List[Dict]) -> List[Dict]:
        """–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ URL (—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ—Ä—è–¥–∫–∞)."""
        out: List[Dict] = []
        seen = set()
        for it in items:
            url = (it.get("url") or "").strip()
            key = url.split("?")[0] if url else None
            if key and key in seen:
                continue
            if key:
                seen.add(key)
            out.append(it)
        return out
    
    @staticmethod
    def _normalize_url(url: str) -> str:
        """–£–±–∏—Ä–∞–µ–º querystring (utm_* –∏ —Ç.–ø.) —á—Ç–æ–±—ã –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è/—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–ª–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ."""
        if not url:
            return url
        parts = urlsplit(url)
        return urlunsplit((parts.scheme, parts.netloc, parts.path, "", ""))
    
    async def _fetch_bybit_announcements(
        self,
        limit: int = 100,
        locale: str = "en-US",
        ann_type: Optional[str] = None,
        tag: Optional[str] = None,
        max_pages: int = 50,
        days_back: int = 60,
    ) -> List[Dict]:
        """
        Bybit official announcements API:
        GET /v5/announcements/index
        
        –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: https://bybit-exchange.github.io/docs/v5/announcement
        """
        timeout = httpx.Timeout(connect=5.0, read=8.0, write=8.0, pool=5.0)
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        base_url = self.exchange_announcement_urls.get("Bybit")
        if not base_url:
            return []
        
        try:
            out: List[Dict] = []
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º UTC –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–∞—Ç
            now_utc = datetime.now(timezone.utc)
            # –ë—É—Ñ–µ—Ä 6 —á–∞—Å–æ–≤, —á—Ç–æ–±—ã –Ω–µ —Ç–µ—Ä—è—Ç—å —Å–æ–±—ã—Ç–∏—è –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ "—Ä–æ–≤–Ω–æ 30 –¥–Ω–µ–π –Ω–∞–∑–∞–¥"
            cutoff_date = now_utc - timedelta(days=days_back, hours=6)
            stop_early = False
            page_limit = min(50, limit)
            
            async with httpx.AsyncClient(timeout=timeout, follow_redirects=True, headers=headers) as client:
                for page in range(1, max_pages + 1):
                    if stop_early:
                        break
                    params: Dict[str, str] = {
                        "locale": locale,
                        "page": str(page),
                        "limit": str(page_limit),  # Bybit –Ω–æ—Ä–º–∞–ª—å–Ω–æ –ø–µ—Ä–µ–≤–∞—Ä–∏–≤–∞–µ—Ç 50
                    }
                    if ann_type:
                        params["type"] = str(ann_type)
                    if tag:
                        params["tag"] = str(tag)
                    
                    r = await client.get(base_url, params=params)
                    if r.status_code != 200:
                        logger.warning("Bybit announcements API –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å %s", r.status_code)
                        break
                    
                    data = r.json()
                    if not isinstance(data, dict) or data.get("retCode") != 0:
                        logger.warning("Bybit announcements API –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É: %s", data.get("retMsg") if isinstance(data, dict) else "bad_json")
                        break
                    
                    result = data.get("result", {})
                    items = result.get("list", []) or []
                    if not items:
                        break
                    
                    for it in items:
                        try:
                            title = (it.get("title") or "").strip()
                            url = self._normalize_url((it.get("url") or "").strip())
                            description = (it.get("description") or "").strip()
                            
                            if not title or not url:
                                continue
                            
                            # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ UTC
                            published_at = now_utc
                            publish_time = it.get("publishTime")
                            if publish_time:
                                try:
                                    published_at = datetime.fromtimestamp(int(str(publish_time)) / 1000, tz=timezone.utc)
                                except Exception:
                                    published_at = now_utc
                            
                            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ (–µ—Å–ª–∏ —ç–ª–µ–º–µ–Ω—Ç —Å—Ç–∞—Ä—à–µ cutoff_date, –º–æ–∂–Ω–æ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å—Å—è)
                            if published_at < cutoff_date:
                                # –°–ø–∏—Å–æ–∫ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–Ω–æ–≤—ã–µ -> —Å—Ç–∞—Ä—ã–µ), –º–æ–∂–Ω–æ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å—Å—è
                                stop_early = True
                                break
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∏–ø –∏ —Ç–µ–≥–∏
                            ann_type_obj = it.get("type", {})
                            type_key = ann_type_obj.get("key", "") if isinstance(ann_type_obj, dict) else ""
                            type_title = ann_type_obj.get("title", "") if isinstance(ann_type_obj, dict) else ""
                            tags_list = it.get("tags", []) or []
                            
                            out.append({
                                "title": title,
                                "body": description[:1000],
                                "url": url,
                                "source": "Bybit",
                                "published_at": published_at,
                                "tags": ["Bybit", "exchange", "announcement", type_key, type_title] + (tags_list if isinstance(tags_list, list) else []),
                            })
                        except Exception:
                            continue
                    
                    out = self._dedupe_by_url(out)
                    if len(out) >= limit:
                        break
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –µ—â–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    total = result.get("total", 0)
                    if page * page_limit >= total:
                        break
                    
                    # –ú—è–≥–∫–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                    await asyncio.sleep(0.05)
            
            out.sort(key=lambda x: x["published_at"], reverse=True)
            out = self._dedupe_by_url(out)[:limit]
            return out
        except Exception as e:
            logger.warning("Bybit announcements API –æ—à–∏–±–∫–∞: %s", e)
            return []
    
    async def _fetch_exchange_announcements(self, limit: int = 100, days_back: int = 60) -> List[Dict]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å –±–∏—Ä–∂
        
        Args:
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π
            days_back: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –±–∏—Ä–∂
        """
        all_news = []
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º UTC –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–∞—Ç
        now_utc = datetime.now(timezone.utc)
        # –ë—É—Ñ–µ—Ä 6 —á–∞—Å–æ–≤, —á—Ç–æ–±—ã –Ω–µ —Ç–µ—Ä—è—Ç—å —Å–æ–±—ã—Ç–∏—è –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ
        lookback = now_utc - timedelta(days=days_back, hours=6)
        
        timeout = httpx.Timeout(connect=5.0, read=8.0, write=8.0, pool=5.0)
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        
        async def _fetch_one(exchange_name: str, base_url: Optional[Union[str, List[str]]]) -> List[Dict]:
            local: List[Dict] = []
            try:
                # Bybit ‚Äî –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π JSON API
                if exchange_name == "Bybit":
                    result = await self._fetch_bybit_announcements(
                        limit=min(limit, 100),
                        days_back=days_back,
                        ann_type="delistings",
                        tag="Derivatives"
                    )
                    return result
                
                # –î–ª—è –±–∏—Ä–∂ –±–µ–∑ –ø—É–±–ª–∏—á–Ω–æ–≥–æ API –∏ –±–µ–∑ URL –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                if base_url is None:
                    logger.debug("‚è≠Ô∏è %s: –ø—Ä–æ–ø—É—â–µ–Ω (–Ω–µ—Ç –ø—É–±–ª–∏—á–Ω–æ–≥–æ REST API –¥–ª—è announcements)", exchange_name)
                    return []
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ URL –∫–∞—Ç–µ–≥–æ—Ä–∏–π
                if isinstance(base_url, str):
                    urls_to_fetch = [base_url]
                elif isinstance(base_url, list):
                    urls_to_fetch = base_url
                else:
                    urls_to_fetch = []
                
                if not urls_to_fetch:
                    return []
                
                async with httpx.AsyncClient(timeout=timeout, follow_redirects=True, headers=headers) as client:
                    for url in urls_to_fetch:
                        try:
                            r = await client.get(url)
                            if r.status_code != 200:
                                logger.debug("üîç %s: announcements %s –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å %s", exchange_name, url, r.status_code)
                                continue
                            
                            soup = BeautifulSoup(r.text, "html.parser")
                            articles: List = []
                            
                            # –û–±—â–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –¥—Ä—É–≥–∏—Ö –±–∏—Ä–∂
                            articles.extend(soup.find_all("a", href=re.compile(r"article|announcement|support|help", re.I)))
                            articles.extend(soup.find_all(["article", "div"], class_=re.compile(r"article|announcement|news|support", re.I)))
                
                            seen_urls = set()
                            for article in articles[: max(10, limit * 2)]:
                                try:
                                    url_elem = article if getattr(article, "name", None) == "a" else article.find("a")
                                    if not url_elem:
                                        continue
                                    href = url_elem.get("href", "") or ""
                                    if not href:
                                        continue
                                    if not href.startswith("http"):
                                        if href.startswith("/"):
                                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—ã–π –¥–æ–º–µ–Ω –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ URL
                                            url_parts = url.split("/")
                                            base_domain = f"{url_parts[0]}//{url_parts[2]}"
                                            href = base_domain + href
                                        else:
                                            href = url.rstrip("/") + "/" + href.lstrip("/")
                                    href = href.split("?")[0]
                                    if href in seen_urls:
                                        continue
                                    seen_urls.add(href)
                        
                                    title_elem = article.find(["h1", "h2", "h3", "h4", "span", "div", "a"], class_=re.compile(r"title|heading|name", re.I))
                                    if not title_elem:
                                        title_elem = url_elem
                                    title = (title_elem.get_text(strip=True) if title_elem else "").strip()
                                    if not title or len(title) < 5:
                                        continue
                                    
                                    body_elem = article.find(["p", "div", "span"], class_=re.compile(r"content|body|description|text|summary", re.I))
                                    body = body_elem.get_text(strip=True)[:500] if body_elem else ""
                                    
                                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è –∫–∞–∫ –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—É—é –¥–∞—Ç—É (UTC)
                                    published_at = datetime.now(timezone.utc)
                                    
                                    local.append(
                                        {
                                            "title": title,
                                            "body": body,
                                            "url": href,
                                            "source": exchange_name,
                                            "published_at": published_at,
                                            "tags": [exchange_name, "exchange", "announcement"],
                                        }
                                    )
                                    if len(local) >= limit:
                                        break
                                except Exception:
                                    continue
                        except Exception as e:
                            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ URL {url} –¥–ª—è {exchange_name}: {e}")
                            continue
                
                if local:
                    logger.debug("  ‚úì %s: –∑–∞–≥—Ä—É–∂–µ–Ω–æ %s –æ–±—ä—è–≤–ª–µ–Ω–∏–π", exchange_name, len(local))
                return local[:limit]
            except Exception as e:
                logger.warning("‚ùå %s: –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ announcements: %s", exchange_name, e)
                return []
        
        tasks = [_fetch_one(name, url) for name, url in self.exchange_announcement_urls.items() if url is not None]
        chunks = await asyncio.gather(*tasks, return_exceptions=False)
        for chunk in chunks:
            all_news.extend(chunk)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        all_news = [n for n in all_news if n["published_at"] > lookback]
        
        return all_news[:limit]
    
    def find_delisting_news(self, news: List[Dict], coin_symbol: str) -> List[Dict]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ –º–æ–Ω–µ—Ç—ã –Ω–∞ –±–∏—Ä–∂–∞—Ö
        
        Args:
            news: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
            coin_symbol: –°–∏–º–≤–æ–ª –º–æ–Ω–µ—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, "FLOW", "BTC")
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ
        """
        coin_upper = coin_symbol.upper()
        relevant_news = []
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥
        delisting_keywords = [
            "delist", "delisting", "removal", "removed", "discontinued", "terminated",
            "trading suspended", "trading halt", "will be delisted", "to be delisted",
            "delisting announcement", "removal from trading", "cease trading",
            "—É–¥–∞–ª–µ–Ω–∏–µ", "–¥–µ–ª–∏—Å—Ç–∏–Ω–≥", "–ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤–ª–∏", "—É–¥–∞–ª–µ–Ω–∏–µ —Å –±–∏—Ä–∂–∏",
            "–ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏–µ –ª–∏—Å—Ç–∏–Ω–≥–∞", "–∏—Å–∫–ª—é—á–µ–Ω–∏–µ –∏–∑ —Ç–æ—Ä–≥–æ–≤–ª–∏"
        ]
        
        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –º–æ–Ω–µ—Ç—ã
        # –§—å—é—á–µ—Ä—Å—ã —Ç–æ–ª—å–∫–æ –∫ USDT, –ø–æ—ç—Ç–æ–º—É –∏—â–µ–º OBOL –∏ OBOLUSDT
        coin_pattern = re.compile(
            rf"(?<![A-Z0-9]){re.escape(coin_upper)}(?:USDT)?(?![A-Z0-9])",
            re.IGNORECASE
        )
        
        for article in news:
            title_body = (article.get("title", "") + " " + article.get("body", "")).upper()
            tags_upper = [str(t).upper() for t in article.get("tags", [])]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –º–æ–Ω–µ—Ç—ã (—Å —É—á–µ—Ç–æ–º —Å—É—Ñ—Ñ–∏–∫—Å–∞ USDT, —Ç–∞–∫ –∫–∞–∫ —Ñ—å—é—á–µ—Ä—Å—ã —Ç–æ–ª—å–∫–æ –∫ USDT)
            # –ù–∞—Ö–æ–¥–∏—Ç OBOL –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ, –∏ OBOLUSDT
            coin_mentioned = coin_pattern.search(title_body) is not None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ –∏–ª–∏ —è–≤–Ω–æ–≥–æ annType=symbol_delisting
            has_delisting_keywords = any(keyword.upper() in title_body for keyword in delisting_keywords) or ("SYMBOL_DELISTING" in tags_upper)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏, –µ—Å–ª–∏ –º–æ–Ω–µ—Ç–∞ —É–ø–æ–º—è–Ω—É—Ç–∞, –Ω–æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω
            if coin_mentioned and not has_delisting_keywords:
                logger.info(f"–ú–æ–Ω–µ—Ç–∞ {coin_symbol} –Ω–∞–π–¥–µ–Ω–∞ –≤ '{article.get('title', '')[:60]}...', –Ω–æ –Ω–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–∞")
            
            if coin_mentioned and has_delisting_keywords:
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥ –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ
                article_with_tag = article.copy()
                if "delisting" not in article_with_tag.get("tags", []):
                    tags = article_with_tag.get("tags", [])
                    tags.append("delisting")
                    article_with_tag["tags"] = tags
                relevant_news.append(article_with_tag)
                # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–π –¥–µ–ª–∏—Å—Ç–∏–Ω–≥ —Å URL
                url = article.get('url', 'N/A')
                logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω –¥–µ–ª–∏—Å—Ç–∏–Ω–≥ {coin_symbol}: {article.get('title', '')[:80]}... | URL: {url}")
        
        return relevant_news
    
    async def check_delisting(self, coin_symbol: str, days_back: int = 60) -> List[Dict]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ –º–æ–Ω–µ—Ç—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π
        
        Args:
            coin_symbol: –°–∏–º–≤–æ–ª –º–æ–Ω–µ—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, "DGRAM", "IOTA")
            days_back: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 60)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ
        """
        logger.info(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–∞ –¥–ª—è {coin_symbol} –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {days_back} –¥–Ω–µ–π...")
        
        # –ü–æ–ª—É—á–∞–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å –±–∏—Ä–∂
        all_announcements = await self._fetch_exchange_announcements(limit=200, days_back=days_back)
        logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(all_announcements)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å –±–∏—Ä–∂")
        
        # –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ
        delisting_news = self.find_delisting_news(all_announcements, coin_symbol)
        
        if not delisting_news:
            # –õ–æ–≥–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ - –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è Bybit –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –º–æ–Ω–µ—Ç—ã
            bybit_announcements = [a for a in all_announcements if a.get("source") == "Bybit"]
            logger.info(f"–î–µ–ª–∏—Å—Ç–∏–Ω–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ {len(bybit_announcements)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π Bybit. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 10 –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏:")
            coin_pattern_debug = re.compile(
                rf"(?<![A-Z0-9]){re.escape(coin_symbol.upper())}(?:USDT)?(?![A-Z0-9])",
                re.IGNORECASE
            )
            for i, ann in enumerate(bybit_announcements[:10]):
                title = ann.get("title", "")[:100]
                body = ann.get("body", "")[:100]
                title_body = (title + " " + body).upper()
                coin_found = coin_pattern_debug.search(title_body) is not None
                logger.info(f"  {i+1}. [{ann.get('source', 'Unknown')}] {title} | –ú–æ–Ω–µ—Ç–∞ –Ω–∞–π–¥–µ–Ω–∞: {coin_found}")
        
        return delisting_news

