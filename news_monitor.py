"""
–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ –º–æ–Ω–µ—Ç —Å –±–∏—Ä–∂
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: Bybit, Gate, MEXC, XT, Binance, Bitget, OKX, BingX
"""
import asyncio
import httpx
from typing import List, Dict, Optional, Union
from datetime import datetime, timedelta, timezone
import logging
import re
from bs4 import BeautifulSoup
from urllib.parse import urlsplit, urlunsplit, urlparse, urljoin

logger = logging.getLogger(__name__)


class NewsMonitor:
    """–ö–ª–∞—Å—Å –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ –º–æ–Ω–µ—Ç"""
    
    def __init__(self):
        # URL —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–¥–¥–µ—Ä–∂–∫–∏/–æ–±—ä—è–≤–ª–µ–Ω–∏–π –±–∏—Ä–∂
        # –î–ª—è –±–∏—Ä–∂ —Å API (Bybit) –∏—Å–ø–æ–ª—å–∑—É–µ–º API. –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö - HTML-—Å–∫—Ä–∞–ø–∏–Ω–≥.
        self.exchange_announcement_urls = {
            # Bybit: –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π API announcements
            # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: https://bybit-exchange.github.io/docs/v5/announcement
            "Bybit": "https://api.bybit.com/v5/announcements/index",
            # MEXC: HTML-—Å–∫—Ä–∞–ø–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            "MEXC": [
                "https://www.mexc.com/ru-RU/announcements/help-faq/deposits-withdrawals-36",
                "https://www.mexc.com/ru-RU/announcements/delistings",
                "https://www.mexc.com/ru-RU/announcements/tag/deposits-withdrawals-36",
            ],
            # Gate.io: HTML-—Å–∫—Ä–∞–ø–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            "Gate": [
                "https://www.gate.com/ru/announcements/deposit-withdrawal",
                "https://www.gate.com/ru/announcements/delisted",
            ],
            # XT.com: HTML-—Å–∫—Ä–∞–ø–∏–Ω–≥ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            "XT": [
                "https://xtsupport.zendesk.com/hc/en-us/sections/360000106872-Announcements",
                "https://www.xt.com/en/support/articles/announcements",
            ],
            # Binance: HTML-—Å–∫—Ä–∞–ø–∏–Ω–≥ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            "Binance": [
                "https://www.binance.com/en/support/announcement",
                "https://www.binance.com/en/support/announcement/c-48",
            ],
            # Bitget: HTML-—Å–∫—Ä–∞–ø–∏–Ω–≥ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            "Bitget": [
                "https://www.bitgetapp.com/support/articles",
                "https://www.bitgetapp.com/support/articles/category/delisting",
            ],
            # OKX: HTML-—Å–∫—Ä–∞–ø–∏–Ω–≥ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            "OKX": [
                "https://www.okx.com/support/hc/en-us/sections/360000030652-Latest-Announcements",
                "https://www.okx.com/support/hc/en-us/categories/115000275432-Announcements",
            ],
            # BingX: HTML-—Å–∫—Ä–∞–ø–∏–Ω–≥ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            "BingX": [
                "https://support.bingx.com/hc/en-us/sections/360000197872-Announcements",
                "https://support.bingx.com/hc/en-us/categories/360000197872-Announcements",
            ],
        }
        
        # –ú–∞–ø–ø–∏–Ω–≥ –Ω–∞–∑–≤–∞–Ω–∏–π –±–∏—Ä–∂ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏—è –≤ —Å–∏—Å—Ç–µ–º–µ
        self.exchange_name_mapping = {
            "bybit": "Bybit",
            "gate": "Gate",
            "mexc": "MEXC",
            "xt": "XT",
            "binance": "Binance",
            "bitget": "Bitget",
            "okx": "OKX",
            "bingx": "BingX",
        }
    
    @staticmethod
    def _dedupe_by_url(items: List[Dict]) -> List[Dict]:
        """–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ URL (—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ—Ä—è–¥–∫–∞)."""
        out: List[Dict] = []
        seen = set()
        for it in items:
            url = (it.get("url") or "").strip()
            key = NewsMonitor._normalize_url(url) if url else None
            if key and key in seen:
                continue
            if key:
                seen.add(key)
            out.append(it)
        return out
    
    @staticmethod
    def _normalize_url(url: str) -> str:
        """–£–±–∏—Ä–∞–µ–º querystring –∏ fragment (utm_*, #hash –∏ —Ç.–ø.) —á—Ç–æ–±—ã –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è/—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–ª–∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ."""
        if not url:
            return url
        parts = urlsplit(url)
        # –£–±–∏—Ä–∞–µ–º query –∏ fragment, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º trailing slash
        path = parts.path.rstrip("/") or "/"
        return urlunsplit((parts.scheme, parts.netloc, path, "", ""))
    
    async def _fetch_bybit_announcements(
        self,
        limit: int = 100,
        locale: str = "en-US",
        ann_type: Optional[str] = None,
        tag: Optional[str] = None,
        max_pages: int = 50,
        days_back: int = 60,
    ) -> List[Dict]:
        """
        Bybit official announcements API:
        GET /v5/announcements/index
        
        –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: https://bybit-exchange.github.io/docs/v5/announcement
        """
        timeout = httpx.Timeout(connect=5.0, read=8.0, write=8.0, pool=5.0)
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        base_url = self.exchange_announcement_urls.get("Bybit")
        if not base_url:
            return []
        
        try:
            out: List[Dict] = []
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º UTC –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–∞—Ç
            now_utc = datetime.now(timezone.utc)
            # –ë—É—Ñ–µ—Ä 6 —á–∞—Å–æ–≤, —á—Ç–æ–±—ã –Ω–µ —Ç–µ—Ä—è—Ç—å —Å–æ–±—ã—Ç–∏—è –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ "—Ä–æ–≤–Ω–æ 30 –¥–Ω–µ–π –Ω–∞–∑–∞–¥"
            cutoff_date = now_utc - timedelta(days=days_back, hours=6)
            stop_early = False
            page_limit = min(50, limit)
            
            async with httpx.AsyncClient(timeout=timeout, follow_redirects=True, headers=headers) as client:
                for page in range(1, max_pages + 1):
                    if stop_early:
                        break
                    params: Dict[str, str] = {
                        "locale": locale,
                        "page": str(page),
                        "limit": str(page_limit),  # Bybit –Ω–æ—Ä–º–∞–ª—å–Ω–æ –ø–µ—Ä–µ–≤–∞—Ä–∏–≤–∞–µ—Ç 50
                    }
                    if ann_type:
                        params["type"] = str(ann_type)
                    if tag:
                        params["tag"] = str(tag)
                    
                    r = await client.get(base_url, params=params)
                    if r.status_code != 200:
                        logger.warning("Bybit announcements API –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å %s", r.status_code)
                        break
                    
                    data = r.json()
                    if not isinstance(data, dict) or data.get("retCode") != 0:
                        logger.warning("Bybit announcements API –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É: %s", data.get("retMsg") if isinstance(data, dict) else "bad_json")
                        break
                    
                    result = data.get("result", {})
                    items = result.get("list", []) or []
                    if not items:
                        break
                    
                    for it in items:
                        try:
                            title = (it.get("title") or "").strip()
                            url = self._normalize_url((it.get("url") or "").strip())
                            description = (it.get("description") or "").strip()
                            
                            if not title or not url:
                                continue
                            
                            # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ UTC
                            published_at = now_utc
                            publish_time = it.get("publishTime")
                            if publish_time:
                                try:
                                    published_at = datetime.fromtimestamp(int(str(publish_time)) / 1000, tz=timezone.utc)
                                except Exception:
                                    published_at = now_utc
                            
                            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ (–µ—Å–ª–∏ —ç–ª–µ–º–µ–Ω—Ç —Å—Ç–∞—Ä—à–µ cutoff_date, –º–æ–∂–Ω–æ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å—Å—è)
                            if published_at < cutoff_date:
                                # –°–ø–∏—Å–æ–∫ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–Ω–æ–≤—ã–µ -> —Å—Ç–∞—Ä—ã–µ), –º–æ–∂–Ω–æ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å—Å—è
                                stop_early = True
                                break
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∏–ø –∏ —Ç–µ–≥–∏
                            ann_type_obj = it.get("type", {})
                            type_key = ann_type_obj.get("key", "") if isinstance(ann_type_obj, dict) else ""
                            type_title = ann_type_obj.get("title", "") if isinstance(ann_type_obj, dict) else ""
                            tags_list = it.get("tags", []) or []
                            
                            out.append({
                                "title": title,
                                "body": description[:1000],
                                "url": url,
                                "source": "Bybit",
                                "published_at": published_at,
                                "tags": ["Bybit", "exchange", "announcement", type_key, type_title] + (tags_list if isinstance(tags_list, list) else []),
                            })
                        except Exception:
                            continue
                    
                    out = self._dedupe_by_url(out)
                    if len(out) >= limit:
                        break
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –µ—â–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    total = result.get("total", 0)
                    if page * page_limit >= total:
                        break
                    
                    # –ú—è–≥–∫–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                    await asyncio.sleep(0.05)
            
            out.sort(key=lambda x: x["published_at"], reverse=True)
            out = self._dedupe_by_url(out)[:limit]
            return out
        except Exception as e:
            logger.warning("Bybit announcements API –æ—à–∏–±–∫–∞: %s", e)
            return []
    
    async def _fetch_exchange_announcements(self, limit: int = 100, days_back: int = 60, exchanges: Optional[List[str]] = None) -> List[Dict]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å –±–∏—Ä–∂
        
        Args:
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π
            days_back: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞
            exchanges: –°–ø–∏—Å–æ–∫ –±–∏—Ä–∂ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ["bybit", "gate"]). –ï—Å–ª–∏ None, –ø—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è –≤—Å–µ –±–∏—Ä–∂–∏.
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –±–∏—Ä–∂
        """
        all_news = []
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º UTC –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–∞—Ç
        now_utc = datetime.now(timezone.utc)
        # –ë—É—Ñ–µ—Ä 6 —á–∞—Å–æ–≤, —á—Ç–æ–±—ã –Ω–µ —Ç–µ—Ä—è—Ç—å —Å–æ–±—ã—Ç–∏—è –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–µ
        lookback = now_utc - timedelta(days=days_back, hours=6)
        
        timeout = httpx.Timeout(connect=5.0, read=8.0, write=8.0, pool=5.0)
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –±–∏—Ä–∂–∏, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã
        exchanges_to_check = self.exchange_announcement_urls
        if exchanges:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –±–∏—Ä–∂ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏—è –≤ —Å–∏—Å—Ç–µ–º–µ
            mapped_exchanges = [self.exchange_name_mapping.get(ex.lower(), ex.capitalize()) for ex in exchanges]
            exchanges_to_check = {name: url for name, url in self.exchange_announcement_urls.items() if name in mapped_exchanges}
        
        async def _fetch_one(exchange_name: str, base_url: Optional[Union[str, List[str]]]) -> List[Dict]:
            local: List[Dict] = []
            try:
                # Bybit ‚Äî –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π JSON API
                if exchange_name == "Bybit":
                    result = await self._fetch_bybit_announcements(
                        limit=min(limit, 200),
                        days_back=days_back,
                        ann_type=None,
                        tag=None,
                    )
                    return result
                
                # –î–ª—è –±–∏—Ä–∂ –±–µ–∑ –ø—É–±–ª–∏—á–Ω–æ–≥–æ API –∏ –±–µ–∑ URL –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                if base_url is None:
                    logger.debug("‚è≠Ô∏è %s: –ø—Ä–æ–ø—É—â–µ–Ω (–Ω–µ—Ç –ø—É–±–ª–∏—á–Ω–æ–≥–æ REST API –¥–ª—è announcements)", exchange_name)
                    return []
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ URL –∫–∞—Ç–µ–≥–æ—Ä–∏–π
                if isinstance(base_url, str):
                    urls_to_fetch = [base_url]
                elif isinstance(base_url, list):
                    urls_to_fetch = base_url
                else:
                    urls_to_fetch = []
                
                if not urls_to_fetch:
                    return []
                
                # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ—Ç—Å–µ–∏–≤–∞–Ω–∏—è –º—É—Å–æ—Ä–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ (–∫–∞—Ç–µ–≥–æ—Ä–∏–∏, —Å–µ–∫—Ü–∏–∏, –ø–æ–∏—Å–∫ –∏ —Ç.–ø.)
                # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –∫ path, –Ω–µ –∫ –ø–æ–ª–Ω–æ–º—É URL
                deny_patterns = [
                    r"/categories?/",
                    r"/sections?/",
                    r"/tag/",
                    r"/search",
                    r"/login",
                    r"/register",
                ]
                deny_re = re.compile("|".join(deny_patterns), re.I)
                
                # seen_urls –Ω–∞ –≤—Å—é –±–∏—Ä–∂—É (–≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏), —á—Ç–æ–±—ã –Ω–µ –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–±–æ—Ç—É
                seen_urls = set()
                
                async with httpx.AsyncClient(timeout=timeout, follow_redirects=True, headers=headers) as client:
                    for url in urls_to_fetch:
                        try:
                            r = await client.get(url)
                            if r.status_code != 200:
                                logger.debug("üîç %s: announcements %s –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å %s", exchange_name, url, r.status_code)
                                continue
                            
                            soup = BeautifulSoup(r.text, "html.parser")
                            articles: List = []
                            
                            # –û–±—â–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –¥—Ä—É–≥–∏—Ö –±–∏—Ä–∂
                            articles.extend(soup.find_all("a", href=re.compile(r"article|announcement|support|help", re.I)))
                            articles.extend(soup.find_all(["article", "div"], class_=re.compile(r"article|announcement|news|support", re.I)))
                            
                            # –ñ—ë—Å—Ç–∫–∏–π –ø–æ—Ç–æ–ª–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç–µ–π (–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å)
                            max_articles = min(2000, max(200, limit * 10))
                            for article in articles[:max_articles]:
                                try:
                                    url_elem = article if getattr(article, "name", None) == "a" else article.find("a")
                                    if not url_elem:
                                        continue
                                    href = url_elem.get("href", "") or ""
                                    if not href:
                                        continue
                                    if not href.startswith("http"):
                                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º urljoin –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–π —Å–±–æ—Ä–∫–∏ URL (–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç edge-cases)
                                        href = urljoin(url, href)
                                    href = href.split("?")[0]
                                    href = self._normalize_url(href)
                                    
                                    # –§–∏–ª—å—Ç—Ä—É–µ–º –º—É—Å–æ—Ä–Ω—ã–µ —Å—Å—ã–ª–∫–∏ (–∫–∞—Ç–µ–≥–æ—Ä–∏–∏, —Å–µ–∫—Ü–∏–∏, –ø–æ–∏—Å–∫ –∏ —Ç.–ø.)
                                    # –ü—Ä–∏–º–µ–Ω—è–µ–º deny —Ç–æ–ª—å–∫–æ –∫ path, —á—Ç–æ–±—ã –Ω–µ –≤—ã–∫–∏–Ω—É—Ç—å –≤–∞–ª–∏–¥–Ω—ã–µ /support/... –∏–ª–∏ /help/...
                                    parsed = urlparse(href)
                                    if deny_re.search(parsed.path):
                                        continue
                                    
                                    if href in seen_urls:
                                        continue
                                    seen_urls.add(href)
                        
                                    title_elem = article.find(["h1", "h2", "h3", "h4", "span", "div", "a"], class_=re.compile(r"title|heading|name", re.I))
                                    if not title_elem:
                                        title_elem = url_elem
                                    title = (title_elem.get_text(strip=True) if title_elem else "").strip()
                                    if not title or len(title) < 5:
                                        continue
                                    
                                    body_elem = article.find(["p", "div", "span"], class_=re.compile(r"content|body|description|text|summary", re.I))
                                    body = body_elem.get_text(strip=True)[:500] if body_elem else ""
                                    
                                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∏–∑ —Å—Ç–∞—Ç—å–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Å–ø–∏—Å–∫–∞
                                    published_at = None
                                    # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –¥–∞—Ç—É –≤ time —ç–ª–µ–º–µ–Ω—Ç–µ —Ä—è–¥–æ–º —Å article
                                    time_elem = article.find("time")
                                    if time_elem:
                                        datetime_attr = time_elem.get("datetime")
                                        if datetime_attr:
                                            try:
                                                # –ü–∞—Ä—Å–∏–º ISO —Ñ–æ—Ä–º–∞—Ç: 2024-01-15T10:30:00Z –∏–ª–∏ 2024-01-15T10:30:00+00:00
                                                if "T" in datetime_attr:
                                                    published_at = datetime.fromisoformat(datetime_attr.replace("Z", "+00:00"))
                                                else:
                                                    published_at = datetime.strptime(datetime_attr, "%Y-%m-%d")
                                                    published_at = published_at.replace(tzinfo=timezone.utc)
                                            except Exception:
                                                pass
                                    
                                    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ time, –∏—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ —Ä—è–¥–æ–º (–º–Ω–æ–≥–∏–µ –±–∏—Ä–∂–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –¥–∞—Ç—É –≤ span/div)
                                    if published_at is None:
                                        date_elem = article.find(["span", "div", "p"], class_=re.compile(r"date|time|published|created", re.I))
                                        if date_elem:
                                            date_text = date_elem.get_text(strip=True)
                                            # –ü—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç
                                            for fmt in ["%Y-%m-%d", "%d.%m.%Y", "%m/%d/%Y", "%Y/%m/%d"]:
                                                try:
                                                    published_at = datetime.strptime(date_text[:10], fmt)
                                                    published_at = published_at.replace(tzinfo=timezone.utc)
                                                    break
                                                except Exception:
                                                    continue
                                    
                                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞—Ç—É –∫ UTC
                                    published_at_inferred = False
                                    if published_at is not None:
                                        if published_at.tzinfo is not None:
                                            published_at = published_at.astimezone(timezone.utc)
                                        else:
                                            # –ï—Å–ª–∏ –¥–∞—Ç–∞ –±–µ–∑ timezone, —Å—á–∏—Ç–∞–µ–º —á—Ç–æ UTC
                                            published_at = published_at.replace(tzinfo=timezone.utc)
                                    else:
                                        # –Ω–µ—Ç –¥–∞—Ç—ã => –æ—Å—Ç–∞–≤–ª—è–µ–º, –∏–Ω–∞—á–µ days_back –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –±–∏—Ä–∂–∞—Ö –±–µ–∑ –¥–∞—Ç –≤ –ª–∏—Å—Ç–∏–Ω–≥–µ
                                        # —Å—Ç–∞–≤–∏–º now_utc —á—Ç–æ–±—ã —ç–ª–µ–º–µ–Ω—Ç –ø—Ä–æ—à—ë–ª —Ñ–∏–ª—å—Ç—Ä, –¥–∞—Ç—É –ø–æ–ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–∏ –¥–æ–≥—Ä—É–∑–µ
                                        published_at = datetime.now(timezone.utc)
                                        published_at_inferred = True
                                    
                                    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ lookback —Å—Ä–∞–∑—É (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)
                                    # –ù–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∏–ª—å—Ç—Ä –¥–ª—è inferred –¥–∞—Ç, —á—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –±–µ–∑ –¥–∞—Ç—ã
                                    if not published_at_inferred and published_at <= lookback:
                                        continue
                                    
                                    local.append(
                                        {
                                            "title": title,
                                            "body": body,
                                            "url": href,
                                            "source": exchange_name,
                                            "published_at": published_at,
                                            "published_at_inferred": published_at_inferred,
                                            "tags": [exchange_name, "exchange", "announcement"],
                                        }
                                    )
                                    if len(local) >= limit:
                                        break
                                except Exception:
                                    continue
                        except Exception as e:
                            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ URL {url} –¥–ª—è {exchange_name}: {e}")
                            continue
                
                if local:
                    logger.debug("  ‚úì %s: –∑–∞–≥—Ä—É–∂–µ–Ω–æ %s –æ–±—ä—è–≤–ª–µ–Ω–∏–π", exchange_name, len(local))
                # –î–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ–º –ø–æ URL –ø–µ—Ä–µ–¥ –≤–æ–∑–≤—Ä–∞—Ç–æ–º
                local = self._dedupe_by_url(local)
                return local[:limit]
            except Exception as e:
                logger.warning("‚ùå %s: –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ announcements: %s", exchange_name, e)
                return []
        
        tasks = [_fetch_one(name, url) for name, url in exchanges_to_check.items() if url is not None]
        chunks = await asyncio.gather(*tasks, return_exceptions=False)
        for chunk in chunks:
            all_news.extend(chunk)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        all_news = [n for n in all_news if n["published_at"] > lookback]
        
        return all_news[:limit]
    
    async def find_delisting_news(self, news: List[Dict], coin_symbol: str) -> List[Dict]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ –º–æ–Ω–µ—Ç—ã –Ω–∞ –±–∏—Ä–∂–∞—Ö
        
        Args:
            news: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
            coin_symbol: –°–∏–º–≤–æ–ª –º–æ–Ω–µ—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, "FLOW", "BTC")
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ
        """
        coin_upper = coin_symbol.upper()
        relevant_news = []
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–∞: —Ä–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ hard (—Ä–µ–∞–ª—å–Ω—ã–π –¥–µ–ª–∏—Å—Ç–∏–Ω–≥) –∏ soft (–≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞—É–∑–∞)
        hard_delisting_keywords = [
            "delist", "delisting", "removal", "removed", "discontinued", "terminated",
            "will be delisted", "to be delisted", "delisting announcement",
            "removal from trading", "cease trading", "termination",
            "—É–¥–∞–ª–µ–Ω–∏–µ", "–¥–µ–ª–∏—Å—Ç–∏–Ω–≥", "–ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤–ª–∏", "—É–¥–∞–ª–µ–Ω–∏–µ —Å –±–∏—Ä–∂–∏",
            "–ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏–µ –ª–∏—Å—Ç–∏–Ω–≥–∞", "–∏—Å–∫–ª—é—á–µ–Ω–∏–µ –∏–∑ —Ç–æ—Ä–≥–æ–≤–ª–∏"
        ]
        # soft_keywords (suspend/halt/pause) - –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—É–∑—ã, –Ω–µ —Å—á–∏—Ç–∞–µ–º –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–æ–º
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ hard-–Ω–∞–±–æ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–∞
        delisting_keywords = hard_delisting_keywords
        
        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –º–æ–Ω–µ—Ç—ã
        # –§—å—é—á–µ—Ä—Å—ã —Ç–æ–ª—å–∫–æ –∫ USDT, –ø–æ—ç—Ç–æ–º—É –∏—â–µ–º OBOL –∏ OBOLUSDT
        coin_pattern = re.compile(
            rf"(?<![A-Z0-9]){re.escape(coin_upper)}(?:USDT)?(?![A-Z0-9])",
            re.IGNORECASE
        )
        
        # –£—Å–ª–æ–≤–Ω—ã–π –¥–æ–≥—Ä—É–∑ —Å—Ç–∞—Ç–µ–π: –¥–æ–≥—Ä—É–∂–∞–µ–º –µ—Å–ª–∏ –º–æ–Ω–µ—Ç–∞ —É–ø–æ–º—è–Ω—É—Ç–∞ –∏–ª–∏ –µ—Å—Ç—å delist-–∫–ª—é—á–∏ –≤ –∫–∞—Ä—Ç–æ—á–∫–µ
        timeout = httpx.Timeout(connect=5.0, read=8.0, write=8.0, pool=5.0)
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
        
        # –ó–∞—â–∏—Ç–Ω—ã–µ –º–µ—Ä—ã –¥–ª—è –¥–æ–≥—Ä—É–∑–∞: –ª–∏–º–∏—Ç, –∫–µ—à (—Ö—Ä–∞–Ω–∏—Ç body –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å)
        fetch_cache: Dict[str, Optional[str]] = {}  # –∫–µ—à –ø–æ URL: body –∏–ª–∏ None (sentinel –¥–ª—è "–Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å")
        fetch_limit = 20  # –¥–æ–≥—Ä—É–∂–∞—Ç—å –º–∞–∫—Å–∏–º—É–º 20 —Å—Ç–∞—Ç–µ–π –Ω–∞ –º–æ–Ω–µ—Ç—É
        
        # –û–¥–∏–Ω –∫–ª–∏–µ–Ω—Ç –Ω–∞ –≤—Å—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
        async with httpx.AsyncClient(timeout=timeout, follow_redirects=True, headers=headers) as client:
            fetch_count = 0
            for article in news:
                title_body = (article.get("title", "") + " " + article.get("body", "")).upper()
                tags_upper = [str(t).upper() for t in article.get("tags", [])]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –º–æ–Ω–µ—Ç—ã (—Å —É—á–µ—Ç–æ–º —Å—É—Ñ—Ñ–∏–∫—Å–∞ USDT, —Ç–∞–∫ –∫–∞–∫ —Ñ—å—é—á–µ—Ä—Å—ã —Ç–æ–ª—å–∫–æ –∫ USDT)
                # –ù–∞—Ö–æ–¥–∏—Ç OBOL –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ, –∏ OBOLUSDT
                coin_mentioned = coin_pattern.search(title_body) is not None
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ –≤ –∫–∞—Ä—Ç–æ—á–∫–µ
                has_delisting_keywords_in_card = any(keyword.upper() in title_body for keyword in delisting_keywords)
                
                # –£—Å–ª–æ–≤–Ω—ã–π –¥–æ–≥—Ä—É–∑: –µ—Å–ª–∏ –º–æ–Ω–µ—Ç–∞ —É–ø–æ–º—è–Ω—É—Ç–∞ –ò–õ–ò –µ—Å—Ç—å delist-–∫–ª—é—á–∏ –≤ –∫–∞—Ä—Ç–æ—á–∫–µ (–¥–∞–∂–µ –±–µ–∑ –º–æ–Ω–µ—Ç—ã)
                # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å "batch delisting" –Ω–æ–≤–æ—Å—Ç–∏, –≥–¥–µ –º–æ–Ω–µ—Ç–∞ —Ç–æ–ª—å–∫–æ –≤–Ω—É—Ç—Ä–∏ —Å—Ç–∞—Ç—å–∏
                should_fetch = (coin_mentioned and not has_delisting_keywords_in_card) or (has_delisting_keywords_in_card and not coin_mentioned)
                
                if should_fetch and fetch_count < fetch_limit:
                    article_url = article.get("url", "")
                    if article_url and article_url.startswith("http"):
                        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL –¥–ª—è –∫–µ—à–∞ (–∫–∞–∫ –≤ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ç–æ—Ä–µ)
                        article_url_normalized = self._normalize_url(article_url)
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
                        if article_url_normalized in fetch_cache:
                            cached_body = fetch_cache[article_url_normalized]
                            if cached_body is not None:  # None –æ–∑–Ω–∞—á–∞–µ—Ç "–Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å", –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å
                                title_body = (article.get("title", "") + " " + cached_body).upper()
                                # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º coin_mentioned –ø–æ—Å–ª–µ –¥–æ–≥—Ä—É–∑–∞
                                coin_mentioned = coin_pattern.search(title_body) is not None
                        else:
                            try:
                                r = await client.get(article_url)
                                fetch_count += 1  # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∏—Ä—É–µ–º –ø–æ—Å–ª–µ –ª—é–±–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                                
                                if r.status_code == 200:
                                    soup_article = BeautifulSoup(r.text, "html.parser")
                                    
                                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –¥–∞—Ç—É –∏–∑ —Å—Ç–∞—Ç—å–∏ (–¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è days_back —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)
                                    published_at_updated = None
                                    time_elem = soup_article.find("time")
                                    if time_elem:
                                        datetime_attr = time_elem.get("datetime")
                                        if datetime_attr:
                                            try:
                                                if "T" in datetime_attr:
                                                    published_at_updated = datetime.fromisoformat(datetime_attr.replace("Z", "+00:00"))
                                                else:
                                                    published_at_updated = datetime.strptime(datetime_attr, "%Y-%m-%d")
                                                    published_at_updated = published_at_updated.replace(tzinfo=timezone.utc)
                                            except Exception:
                                                pass
                                    
                                    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ time, –ø—Ä–æ–±—É–µ–º meta —Ç–µ–≥–∏
                                    if published_at_updated is None:
                                        meta_published = soup_article.find("meta", property="article:published_time") or soup_article.find("meta", attrs={"name": "article:published_time"})
                                        if meta_published:
                                            content = meta_published.get("content", "")
                                            if content:
                                                try:
                                                    published_at_updated = datetime.fromisoformat(content.replace("Z", "+00:00"))
                                                except Exception:
                                                    pass
                                    
                                    # –û–±–Ω–æ–≤–ª—è–µ–º published_at –≤ —Å—Ç–∞—Ç—å–µ, –µ—Å–ª–∏ –Ω–∞—à–ª–∏ –¥–∞—Ç—É
                                    if published_at_updated is not None:
                                        if published_at_updated.tzinfo is not None:
                                            published_at_updated = published_at_updated.astimezone(timezone.utc)
                                        else:
                                            published_at_updated = published_at_updated.replace(tzinfo=timezone.utc)
                                        article["published_at"] = published_at_updated
                                        article["published_at_inferred"] = False
                                    
                                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
                                    main_content = soup_article.find("main") or soup_article.find("article") or soup_article.find("div", class_=re.compile(r"content|article|body", re.I))
                                    if main_content:
                                        body_full = main_content.get_text(strip=True)[:2000]  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
                                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
                                        fetch_cache[article_url_normalized] = body_full
                                        # –ù–ï –º—É—Ç–∏—Ä—É–µ–º article, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                                        title_body = (article.get("title", "") + " " + body_full).upper()
                                        # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º coin_mentioned –ø–æ—Å–ª–µ –¥–æ–≥—Ä—É–∑–∞
                                        coin_mentioned = coin_pattern.search(title_body) is not None
                                    else:
                                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º None –∫–∞–∫ sentinel - –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å
                                        fetch_cache[article_url_normalized] = None
                            except Exception as e:
                                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–≥—Ä—É–∑–∏—Ç—å —Å—Ç–∞—Ç—å—é {article_url}: {e}")
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º None –≤ –∫–µ—à, —á—Ç–æ–±—ã –Ω–µ –ø–æ–≤—Ç–æ—Ä—è—Ç—å –∑–∞–ø—Ä–æ—Å
                                fetch_cache[article_url_normalized] = None
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ –∏–ª–∏ —è–≤–Ω–æ–≥–æ annType=symbol_delisting
                has_delisting_keywords = any(keyword.upper() in title_body for keyword in delisting_keywords) or ("SYMBOL_DELISTING" in tags_upper)
                
                # –õ–æ–≥–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏, –µ—Å–ª–∏ –º–æ–Ω–µ—Ç–∞ —É–ø–æ–º—è–Ω—É—Ç–∞, –Ω–æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω
                if coin_mentioned and not has_delisting_keywords:
                    logger.info(f"–ú–æ–Ω–µ—Ç–∞ {coin_symbol} –Ω–∞–π–¥–µ–Ω–∞ –≤ '{article.get('title', '')[:60]}...', –Ω–æ –Ω–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–∞")
                
                if coin_mentioned and has_delisting_keywords:
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥ –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ
                    article_with_tag = article.copy()
                    if "delisting" not in article_with_tag.get("tags", []):
                        tags = article_with_tag.get("tags", [])
                        tags.append("delisting")
                        article_with_tag["tags"] = tags
                    relevant_news.append(article_with_tag)
                    # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–π –¥–µ–ª–∏—Å—Ç–∏–Ω–≥ —Å URL
                    url = article.get('url', 'N/A')
                    logger.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω –¥–µ–ª–∏—Å—Ç–∏–Ω–≥ {coin_symbol}: {article.get('title', '')[:80]}... | URL: {url}")
        
        return relevant_news
    
    async def check_delisting(self, coin_symbol: str, exchanges: Optional[List[str]] = None, days_back: int = 60) -> List[Dict]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ –º–æ–Ω–µ—Ç—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π
        
        Args:
            coin_symbol: –°–∏–º–≤–æ–ª –º–æ–Ω–µ—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, "DGRAM", "IOTA")
            exchanges: –°–ø–∏—Å–æ–∫ –±–∏—Ä–∂ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ["bybit", "gate"]). –ï—Å–ª–∏ None, –ø—Ä–æ–≤–µ—Ä—è—é—Ç—Å—è –≤—Å–µ –±–∏—Ä–∂–∏. –ï—Å–ª–∏ [], –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è.
            days_back: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 60)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ
        """
        # [] => —è–≤–Ω–æ –Ω–∏—á–µ–≥–æ –Ω–µ –ø—Ä–æ–≤–µ—Ä—è—Ç—å
        if exchanges == []:
            return []
        
        # –ü–æ–ª—É—á–∞–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å –±–∏—Ä–∂ (None => –≤—Å–µ –±–∏—Ä–∂–∏)
        all_announcements = await self._fetch_exchange_announcements(limit=200, days_back=days_back, exchanges=exchanges)
        
        # –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –æ –¥–µ–ª–∏—Å—Ç–∏–Ω–≥–µ (—Ç–µ–ø–µ—Ä—å async –¥–ª—è —É—Å–ª–æ–≤–Ω–æ–≥–æ –¥–æ–≥—Ä—É–∑–∞ —Å—Ç–∞—Ç–µ–π)
        delisting_news = await self.find_delisting_news(all_announcements, coin_symbol)
        
        return delisting_news

